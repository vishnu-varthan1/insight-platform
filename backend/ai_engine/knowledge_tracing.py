"""
AMEP Knowledge Tracing Engine
Implements Hybrid Model: BKT + DKT + DKVMN
Solves: BR1 (Personalized Mastery), BR2 (Adaptive Practice), BR3 (Efficiency)

Research Sources:
- Paper 2105_15106v4.pdf: Knowledge Tracing Survey
- Paper 4.pdf: AI-Powered Personalized Learning
- Paper 6.pdf: Adaptive Learning Pathways
"""

import numpy as np
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

# ============================================================================
# LAYER 1: BAYESIAN KNOWLEDGE TRACING (BKT) - For Interpretability
# ============================================================================

@dataclass
class BKTParameters:
    """
    BKT Model Parameters (BR1: Continuous Mastery Scoring)
    
    P(L0): Prior probability of knowledge (initial mastery)
    P(T): Probability of learning transition
    P(G): Probability of guessing correctly
    P(S): Probability of slipping (mistake despite mastery)
    """
    p_l0: float = 0.3   # Initial mastery probability
    p_t: float = 0.2    # Learning rate
    p_g: float = 0.25   # Guess rate (1/4 for multiple choice)
    p_s: float = 0.1    # Slip rate

class BKTEngine:
    """
    Bayesian Knowledge Tracing Implementation
    
    Formula from Paper 2105_15106v4.pdf:
    P(Ln) = P(Ln|Answer) + (1 - P(Ln|Answer)) × P(T)
    P(Cn+1) = P(Ln)(1 - P(S)) + (1 - P(Ln)) × P(G)
    """
    
    def __init__(self, params: BKTParameters = BKTParameters()):
        self.params = params
    
    def update_mastery(
        self, 
        current_mastery: float, 
        is_correct: bool
    ) -> float:
        """
        Update mastery probability based on student response
        
        Returns: Updated mastery score (0-100 scale)
        """
        p_l = current_mastery / 100.0  # Convert to probability
        
        if is_correct:
            # P(L|correct) using Bayes' theorem
            numerator = p_l * (1 - self.params.p_s)
            denominator = (
                p_l * (1 - self.params.p_s) + 
                (1 - p_l) * self.params.p_g
            )
            p_l_given_correct = numerator / denominator if denominator > 0 else p_l
            
            # Apply learning transition
            p_l_new = p_l_given_correct + (1 - p_l_given_correct) * self.params.p_t
        else:
            # P(L|incorrect) using Bayes' theorem
            numerator = p_l * self.params.p_s
            denominator = (
                p_l * self.params.p_s + 
                (1 - p_l) * (1 - self.params.p_g)
            )
            p_l_given_incorrect = numerator / denominator if denominator > 0 else p_l
            
            # Apply learning transition
            p_l_new = p_l_given_incorrect + (1 - p_l_given_incorrect) * self.params.p_t
        
        # Convert back to 0-100 scale
        return min(100.0, max(0.0, p_l_new * 100))

# ============================================================================
# LAYER 2: DEEP KNOWLEDGE TRACING (DKT) - Simulated with Pattern Recognition
# ============================================================================

class DKTEngine:
    """
    Deep Knowledge Tracing (Simplified LSTM Logic)
    
    In production, this would use PyTorch/TensorFlow LSTM networks.
    For prototype: Pattern-based simulation that captures temporal dependencies.
    
    Formula concept from Paper 2105_15106v4.pdf:
    h(t) = tanh(W_hs·x(t) + W_hh·h(t-1) + b_h)
    y(t) = σ(W_yh·h(t) + b_y)
    """
    
    def __init__(self, sequence_length: int = 10):
        self.sequence_length = sequence_length
        self.history_weight = 0.7  # Weight for historical performance
        self.trend_weight = 0.3    # Weight for recent trend
    
    def analyze_pattern(
        self, 
        response_history: List[Dict[str, any]]
    ) -> Dict[str, float]:
        """
        Analyze response patterns to predict mastery
        
        Returns: {
            'predicted_mastery': float,
            'confidence': float,
            'learning_velocity': float
        }
        """
        if not response_history:
            return {
                'predicted_mastery': 30.0,
                'confidence': 0.3,
                'learning_velocity': 0.0
            }
        
        # Extract recent sequence
        recent = response_history[-self.sequence_length:]
        
        # Calculate accuracy trend
        accuracies = [r['is_correct'] for r in recent]
        overall_accuracy = np.mean(accuracies) * 100
        
        # Calculate learning velocity (improvement rate)
        if len(accuracies) >= 3:
            first_half = np.mean(accuracies[:len(accuracies)//2])
            second_half = np.mean(accuracies[len(accuracies)//2:])
            velocity = (second_half - first_half) * 100
        else:
            velocity = 0.0
        
        # Calculate response time patterns (faster = more mastery)
        if all('response_time' in r for r in recent):
            times = [r['response_time'] for r in recent]
            avg_time = np.mean(times)
            time_trend = -np.polyfit(range(len(times)), times, 1)[0]
            # Normalize: faster responses = higher mastery
            time_factor = max(0, min(20, 20 - avg_time/2))
        else:
            time_factor = 0
        
        # Combine factors
        predicted_mastery = min(100, (
            overall_accuracy * self.history_weight +
            (50 + velocity * 2) * self.trend_weight +
            time_factor
        ))
        
        # Confidence based on data volume
        confidence = min(1.0, len(recent) / self.sequence_length)
        
        return {
            'predicted_mastery': predicted_mastery,
            'confidence': confidence,
            'learning_velocity': velocity
        }

# ============================================================================
# LAYER 3: MEMORY-AWARE KNOWLEDGE TRACING (DKVMN) - Concept Relationships
# ============================================================================

class DKVMNEngine:
    """
    Dynamic Key-Value Memory Network (Simplified)
    
    Tracks which concepts are mastered vs. need work (BR3: Efficiency)
    
    Formula from Paper 2105_15106v4.pdf:
    w(t) = Softmax(k(t)·M_k)  -- Correlation weight
    r(t) = Σ w(t,i)·M_v(i)    -- Read operation
    M_v(i) = M_v(i) + w(t,i)·add(t)  -- Write operation
    """
    
    def __init__(self, memory_size: int = 50):
        self.memory_size = memory_size
        # Key memory: Concept relationships
        self.key_memory = {}
        # Value memory: Mastery states
        self.value_memory = {}
    
    def read_mastery(self, concept_id: str, related_concepts: List[str]) -> float:
        """
        Read mastery considering related concepts
        
        BR3: Identifies what's mastered vs. what needs work
        """
        if concept_id not in self.value_memory:
            return 30.0  # Default initial mastery
        
        # Direct mastery
        direct_mastery = self.value_memory[concept_id]
        
        # Weighted contribution from related concepts
        related_mastery = []
        for rel_concept in related_concepts:
            if rel_concept in self.value_memory:
                weight = self._calculate_correlation(concept_id, rel_concept)
                related_mastery.append(self.value_memory[rel_concept] * weight)
        
        if related_mastery:
            # Combine direct and related mastery
            return 0.7 * direct_mastery + 0.3 * np.mean(related_mastery)
        
        return direct_mastery
    
    def write_mastery(
        self, 
        concept_id: str, 
        mastery_update: float,
        related_concepts: List[str]
    ):
        """
        Update mastery and propagate to related concepts
        """
        # Update primary concept
        if concept_id not in self.value_memory:
            self.value_memory[concept_id] = 30.0
        
        self.value_memory[concept_id] = mastery_update
        
        # Store relationship keys
        for rel_concept in related_concepts:
            key = f"{concept_id}_{rel_concept}"
            if key not in self.key_memory:
                self.key_memory[key] = 0.5  # Default correlation
    
    def _calculate_correlation(self, concept_a: str, concept_b: str) -> float:
        """Calculate correlation weight between concepts"""
        key = f"{concept_a}_{concept_b}"
        return self.key_memory.get(key, 0.3)
    
    def get_mastered_concepts(self, threshold: float = 85.0) -> List[str]:
        """
        BR3: Identify mastered concepts to skip
        """
        return [
            concept for concept, mastery in self.value_memory.items()
            if mastery >= threshold
        ]
    
    def get_weak_concepts(self, threshold: float = 60.0) -> List[str]:
        """
        BR3: Identify weak concepts needing focus
        """
        return [
            concept for concept, mastery in self.value_memory.items()
            if mastery < threshold
        ]

# ============================================================================
# HYBRID MODEL ORCHESTRATOR
# ============================================================================

class HybridKnowledgeTracing:
    """
    Combines BKT (interpretability) + DKT (accuracy) + DKVMN (efficiency)
    
    Solves BR1, BR2, BR3 comprehensively
    """
    
    def __init__(self):
        self.bkt = BKTEngine()
        self.dkt = DKTEngine()
        self.dkvmn = DKVMNEngine()
    
    def calculate_mastery(
        self,
        student_id: str,
        concept_id: str,
        is_correct: bool,
        response_time: float,
        current_mastery: float,
        response_history: List[Dict],
        related_concepts: List[str]
    ) -> Dict[str, any]:
        """
        Calculate updated mastery using all three models
        
        Returns comprehensive mastery assessment
        """
        # Layer 1: BKT update (interpretable)
        bkt_mastery = self.bkt.update_mastery(current_mastery, is_correct)
        
        # Layer 2: DKT pattern analysis
        dkt_analysis = self.dkt.analyze_pattern(response_history)
        
        # Layer 3: DKVMN memory-aware adjustment
        dkvmn_mastery = self.dkvmn.read_mastery(concept_id, related_concepts)
        
        # Weighted combination (adjustable based on confidence)
        confidence = dkt_analysis['confidence']
        final_mastery = (
            bkt_mastery * 0.4 +
            dkt_analysis['predicted_mastery'] * (0.4 * confidence) +
            dkvmn_mastery * (0.2 + 0.2 * (1 - confidence))
        )
        
        # Update DKVMN memory
        self.dkvmn.write_mastery(concept_id, final_mastery, related_concepts)
        
        return {
            'mastery_score': round(final_mastery, 2),  # BR1: 0-100 scoring
            'bkt_component': round(bkt_mastery, 2),
            'dkt_component': round(dkt_analysis['predicted_mastery'], 2),
            'dkvmn_component': round(dkvmn_mastery, 2),
            'confidence': round(confidence, 2),
            'learning_velocity': round(dkt_analysis['learning_velocity'], 2),
            'needs_practice': final_mastery < 85.0,  # BR3: Efficiency threshold
            'recommendation': self._get_recommendation(final_mastery)
        }
    
    def _get_recommendation(self, mastery: float) -> str:
        """
        BR3: Efficiency optimization recommendations
        """
        if mastery >= 85.0:
            return "SKIP - Already mastered, move to next concept"
        elif mastery >= 60.0:
            return "LIGHT_REVIEW - 1-2 questions for maintenance"
        else:
            return "FOCUSED_PRACTICE - 5-10 questions with scaffolding"

# ============================================================================
# EXAMPLE USAGE
# ============================================================================

if __name__ == "__main__":
    # Initialize hybrid model
    kt_engine = HybridKnowledgeTracing()
    
    # Sample student response
    result = kt_engine.calculate_mastery(
        student_id="student_001",
        concept_id="algebra_linear_equations",
        is_correct=True,
        response_time=12.5,
        current_mastery=72.0,
        response_history=[
            {'is_correct': False, 'response_time': 25.0},
            {'is_correct': True, 'response_time': 18.0},
            {'is_correct': True, 'response_time': 15.0},
            {'is_correct': True, 'response_time': 12.0},
        ],
        related_concepts=["algebra_variables", "algebra_solving"]
    )
    
    print("Mastery Assessment:")
    print(f"Final Score: {result['mastery_score']}/100")
    print(f"Confidence: {result['confidence']}")
    print(f"Recommendation: {result['recommendation']}")